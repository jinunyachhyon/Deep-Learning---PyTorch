{"cells":[{"source":"# Convolutional Neural Network","metadata":{},"cell_type":"markdown","id":"3d678b5b-4979-4ce8-b842-5a50448176ff"},{"source":"![image-14](image-14.png)\n","metadata":{},"cell_type":"markdown","id":"9fb31525-582b-4375-8ec7-5359239bf7f2"},{"source":"Convolutional Layer reduces the size of image while maintaining the relationship.\n\n![image-15](image-15.png)\n","metadata":{},"cell_type":"markdown","id":"3f84825b-d110-48d4-969f-eabb8cf94344"},{"source":"### Max Pooling \n- Reduces the computational cost by reducing the size of image, reduces the parameters the model has to learn and avoids overfitting by providing the abstract of an image.\n![image-16](image-16.png)\n","metadata":{},"cell_type":"markdown","id":"689d9a17-2a89-44b5-8722-2eafad358396"},{"source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"executionTime":2848,"lastSuccessfullyExecutedCode":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np"},"cell_type":"code","id":"a1717c73-f947-4979-8584-4ed8c8087b09","execution_count":2,"outputs":[]},{"source":"# Hyperparameters\nnum_epochs = 4\nbatch_size = 4\nlearning_rate = 0.001","metadata":{"executionTime":29,"lastSuccessfullyExecutedCode":"# Hyperparameters\nnum_epochs = 4\nbatch_size = 4\nlearning_rate = 0.001"},"cell_type":"code","id":"91bc1562-4649-4d82-8f7c-c24673bb74c0","execution_count":3,"outputs":[]},{"source":"Normalize does the following for each channel:\n\nimage = (image - mean) / std\n\nThe parameters mean, std are passed as 0.5, 0.5 in your case. This will normalize the image in the range [-1,1]. For example, the minimum value 0 will be converted to (0-0.5)/0.5=-1, the maximum value of 1 will be converted to (1-0.5)/0.5=1.\n\nif you would like to get your image back in [0,1] range, you could use,\n\nimage = ((image * std) + mean)","metadata":{},"cell_type":"markdown","id":"c89a4ae7-5dce-4d92-b7a5-c812ede23cbf"},{"source":"# dataset has images range [0,1]\n# so transform them to nonrmalized range [-1,1]\ntransform = transforms.Compose([transforms.ToTensor(),\n                               transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])","metadata":{"executionTime":25,"lastSuccessfullyExecutedCode":"# dataset has images range [0,1]\n# so transform them to nonrmalized range [-1,1]\ntransform = transforms.Compose([transforms.ToTensor(),\n                               transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"},"cell_type":"code","id":"80db896d-2e91-4556-bd5e-7a9d8da26c37","execution_count":13,"outputs":[]},{"source":"# Loading dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data',\n                                            train=True,\n                                            download=True,\n                                            transform=transform)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='./data',\n                                            train=False,\n                                            download=True,\n                                            transform=transform)","metadata":{"executionTime":1434,"lastSuccessfullyExecutedCode":"# Loading dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data',\n                                            train=True,\n                                            download=True,\n                                            transform=transform)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='./data',\n                                            train=False,\n                                            download=True,\n                                            transform=transform)"},"cell_type":"code","id":"fcc967ec-6414-4550-8a5c-842fd4a4623a","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n"}]},{"source":"train_dataset[0][0].shape","metadata":{"executionTime":31,"lastSuccessfullyExecutedCode":"train_dataset[0][0].shape"},"cell_type":"code","id":"70687ae5-9198-4271-824b-5001a8267aaa","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"torch.Size([3, 32, 32])"},"metadata":{}}]},{"source":"train_dataset[0]","metadata":{"executionTime":71,"lastSuccessfullyExecutedCode":"train_dataset[0]"},"cell_type":"code","id":"3ae7e400-056a-4ef3-aaa8-69c3b7cdbb65","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"(tensor([[[-0.5373, -0.6627, -0.6078,  ...,  0.2392,  0.1922,  0.1608],\n          [-0.8745, -1.0000, -0.8588,  ..., -0.0353, -0.0667, -0.0431],\n          [-0.8039, -0.8745, -0.6157,  ..., -0.0745, -0.0588, -0.1451],\n          ...,\n          [ 0.6314,  0.5765,  0.5529,  ...,  0.2549, -0.5608, -0.5843],\n          [ 0.4118,  0.3569,  0.4588,  ...,  0.4431, -0.2392, -0.3490],\n          [ 0.3882,  0.3176,  0.4039,  ...,  0.6941,  0.1843, -0.0353]],\n \n         [[-0.5137, -0.6392, -0.6235,  ...,  0.0353, -0.0196, -0.0275],\n          [-0.8431, -1.0000, -0.9373,  ..., -0.3098, -0.3490, -0.3176],\n          [-0.8118, -0.9451, -0.7882,  ..., -0.3412, -0.3412, -0.4275],\n          ...,\n          [ 0.3333,  0.2000,  0.2627,  ...,  0.0431, -0.7569, -0.7333],\n          [ 0.0902, -0.0353,  0.1294,  ...,  0.1608, -0.5137, -0.5843],\n          [ 0.1294,  0.0118,  0.1137,  ...,  0.4431, -0.0745, -0.2784]],\n \n         [[-0.5059, -0.6471, -0.6627,  ..., -0.1529, -0.2000, -0.1922],\n          [-0.8431, -1.0000, -1.0000,  ..., -0.5686, -0.6078, -0.5529],\n          [-0.8353, -1.0000, -0.9373,  ..., -0.6078, -0.6078, -0.6706],\n          ...,\n          [-0.2471, -0.7333, -0.7961,  ..., -0.4510, -0.9451, -0.8431],\n          [-0.2471, -0.6706, -0.7647,  ..., -0.2627, -0.7333, -0.7333],\n          [-0.0902, -0.2627, -0.3176,  ...,  0.0980, -0.3412, -0.4353]]]),\n 6)"},"metadata":{}}]},{"source":"train_dataset[0][1]","metadata":{"executionTime":409,"lastSuccessfullyExecutedCode":"train_dataset[0][1]"},"cell_type":"code","id":"ef3ec6a5-b39b-4f60-a3c3-566cd232e937","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"6"},"metadata":{}}]},{"source":"# DataLoader object\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)","metadata":{"executionTime":45,"lastSuccessfullyExecutedCode":"# DataLoader object\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)"},"cell_type":"code","id":"b7257434-2392-4b90-9e2d-a099963d1907","execution_count":48,"outputs":[]},{"source":"train_loader","metadata":{"executionTime":49,"lastSuccessfullyExecutedCode":"train_loader"},"cell_type":"code","id":"dea86344-0c8b-43a5-949d-c9eafa23b4e9","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7f16c3046a60>"},"metadata":{}}]},{"source":"classes = ('plane','car','bird','cat',\n          'deer','dog','frog','horse','ship','truck')","metadata":{"executionTime":28,"lastSuccessfullyExecutedCode":"classes = ('plane','car','bird','cat',\n          'deer','dog','frog','horse','ship','truck')"},"cell_type":"code","id":"4cc35f6a-3299-49d0-a95b-bb20132f5348","execution_count":50,"outputs":[]},{"source":"# example\ntrain = iter(train_loader)\nsamples, labels = next(train)\nsamples.shape","metadata":{"executionTime":94,"lastSuccessfullyExecutedCode":"# example\ntrain = iter(train_loader)\nsamples, labels = next(train)\nsamples.shape"},"cell_type":"code","id":"c347dd77-67a7-472e-94d6-d92536a38a6a","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"torch.Size([4, 3, 32, 32])"},"metadata":{}}]},{"source":"samples[0] #4 images in a batch","metadata":{"executionTime":47,"lastSuccessfullyExecutedCode":"samples[0] #4 images in a batch"},"cell_type":"code","id":"70ba49b4-bb6e-44ea-9990-dd3fe554fbdd","execution_count":69,"outputs":[{"output_type":"execute_result","execution_count":69,"data":{"text/plain":"tensor([[[-0.5137, -0.5137, -0.5059,  ..., -0.6784, -0.7176, -0.7412],\n         [-0.4039, -0.4039, -0.4039,  ..., -0.5765, -0.6235, -0.6549],\n         [-0.2941, -0.3020, -0.3020,  ..., -0.4745, -0.5294, -0.5608],\n         ...,\n         [ 0.6863,  0.2471,  0.3020,  ...,  0.8431,  0.8275,  0.8275],\n         [ 0.6627,  0.4667,  0.6784,  ...,  0.8431,  0.8275,  0.8275],\n         [ 0.8118,  0.8118,  0.8353,  ...,  0.8353,  0.8353,  0.8275]],\n\n        [[ 0.2392,  0.2235,  0.2392,  ...,  0.1608,  0.1529,  0.1451],\n         [ 0.2706,  0.2627,  0.2627,  ...,  0.2000,  0.1765,  0.1686],\n         [ 0.3020,  0.2863,  0.2784,  ...,  0.2392,  0.2000,  0.2000],\n         ...,\n         [ 0.6706,  0.2549,  0.3255,  ...,  0.8275,  0.8118,  0.8118],\n         [ 0.6549,  0.4745,  0.7020,  ...,  0.8275,  0.8118,  0.8118],\n         [ 0.8039,  0.8118,  0.8431,  ...,  0.8196,  0.8196,  0.8118]],\n\n        [[ 0.5686,  0.5529,  0.5686,  ...,  0.5294,  0.5137,  0.5059],\n         [ 0.5765,  0.5608,  0.5608,  ...,  0.5608,  0.5294,  0.5137],\n         [ 0.5765,  0.5608,  0.5529,  ...,  0.5843,  0.5451,  0.5294],\n         ...,\n         [ 0.6627,  0.2549,  0.3490,  ...,  0.8353,  0.8196,  0.8196],\n         [ 0.6471,  0.4745,  0.7176,  ...,  0.8353,  0.8196,  0.8196],\n         [ 0.8039,  0.8196,  0.8510,  ...,  0.8275,  0.8275,  0.8196]]])"},"metadata":{}}]},{"source":"samples[0][0] #single image","metadata":{"executionTime":54,"lastSuccessfullyExecutedCode":"samples[0][0] #single image"},"cell_type":"code","id":"6925b3fe-761f-4d02-ba20-d5e7d98ebec7","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"tensor([[-0.5137, -0.5137, -0.5059,  ..., -0.6784, -0.7176, -0.7412],\n        [-0.4039, -0.4039, -0.4039,  ..., -0.5765, -0.6235, -0.6549],\n        [-0.2941, -0.3020, -0.3020,  ..., -0.4745, -0.5294, -0.5608],\n        ...,\n        [ 0.6863,  0.2471,  0.3020,  ...,  0.8431,  0.8275,  0.8275],\n        [ 0.6627,  0.4667,  0.6784,  ...,  0.8431,  0.8275,  0.8275],\n        [ 0.8118,  0.8118,  0.8353,  ...,  0.8353,  0.8353,  0.8275]])"},"metadata":{}}]},{"source":"plt.imshow(samples[0][0])","metadata":{"executionTime":403,"lastSuccessfullyExecutedCode":"plt.imshow(samples[0][0])"},"cell_type":"code","id":"bdd97682-897e-4ec6-88ba-caa3f4e7915b","execution_count":65,"outputs":[{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f1699f531f0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"source":"## Implement Convolution Net\n\n![image-17](image-17.png)\n","metadata":{},"cell_type":"markdown","id":"1e05eadf-8be8-4a37-b518-8ff5962bb9cc"},{"source":"- in_channels (int) – Number of channels in the input image\n- out_channels (int) – Number of channels produced by the convolution\n- kernel_size (int or tuple) – Size of the convolving kernel","metadata":{},"cell_type":"markdown","id":"3f8b4b8f-7eac-4fb4-a762-c46f688430d9"},{"source":"![image-18](image-18.png)\n","metadata":{},"cell_type":"markdown","id":"17928e21-d624-4132-a456-06f2ab30b752"},{"source":"# images in a batch \nimage = samples\nprint(image.shape)\n\nconv1 = nn.Conv2d(3,6,5)\npool = nn.MaxPool2d(2,2)\nconv2 = nn.Conv2d(6,16,5)\n\nx = conv1(image)\nprint(x.shape) #input=32*32,filter=5*5,output=(32-5+0)/1 + 1 = 28*28\n\nx = pool(x)\nprint(x.shape) #output pool by factor of 2 = 14*14\n\nx = conv2(x)\nprint(x.shape) #output = (14-5+0)/1 + 1 = 10*10\n\nx = pool(x)\nprint(x.shape) #output = 5*5","metadata":{"executionTime":44,"lastSuccessfullyExecutedCode":"# images in a batch \nimage = samples\nprint(image.shape)\n\nconv1 = nn.Conv2d(3,6,5)\npool = nn.MaxPool2d(2,2)\nconv2 = nn.Conv2d(6,16,5)\n\nx = conv1(image)\nprint(x.shape) #input=32*32,filter=5*5,output=(32-5+0)/1 + 1 = 28*28\n\nx = pool(x)\nprint(x.shape) #output pool by factor of 2 = 14*14\n\nx = conv2(x)\nprint(x.shape) #output = (14-5+0)/1 + 1 = 10*10\n\nx = pool(x)\nprint(x.shape) #output = 5*5"},"cell_type":"code","id":"1e65d067-0fa5-4399-b0ec-0ce670ec4bd6","execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":"torch.Size([4, 3, 32, 32])\ntorch.Size([4, 6, 28, 28])\ntorch.Size([4, 6, 14, 14])\ntorch.Size([4, 16, 10, 10])\ntorch.Size([4, 16, 5, 5])\n"}]},{"source":"# implement convolution net\nclass ConvNet(nn.Module):\n    \n    def __init__(self):\n        # define layers\n        super(ConvNet,self).__init__()\n        \n        self.conv1 = nn.Conv2d(3,6,5) #(in_channels, out_channels, kernel_size)\n        self.pool = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        \n        # output from conv. layer = torch.Size([4, 16, 5, 5])\n        # this output needs to be flattened to give input\n        self.fc1 = nn.Linear(16*5*5, 120) #flatten, so resize 16*5*5\n        self.fc2 = nn.Linear(120,84)\n        self.fc3 = nn.Linear(84,10)\n        \n    def forward(self, x):\n        # forward pass with activation function\n        \n        x = self.pool(F.relu(self.conv1(x))) #conv layer, activation then pooling\n        x = self.pool(F.relu(self.conv2(x)))\n        \n        x = x.view(-1, 16*5*5) # flatten\n        x = F.relu(self.fc1(x)) #layer1\n        x = F.relu(self.fc2(x)) #layer2\n        x = self.fc3(x) #layer3\n        \n        return x\n        ","metadata":{"executionTime":22,"lastSuccessfullyExecutedCode":"# implement convolution net\nclass ConvNet(nn.Module):\n    \n    def __init__(self):\n        # define layers\n        super(ConvNet,self).__init__()\n        \n        self.conv1 = nn.Conv2d(3,6,5) #(in_channels, out_channels, kernel_size)\n        self.pool = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        \n        # output from conv. layer = torch.Size([4, 16, 5, 5])\n        # this output needs to be flattened to give input\n        self.fc1 = nn.Linear(16*5*5, 120) #flatten, so resize 16*5*5\n        self.fc2 = nn.Linear(120,84)\n        self.fc3 = nn.Linear(84,10)\n        \n    def forward(self, x):\n        # forward pass with activation function\n        \n        x = self.pool(F.relu(self.conv1(x))) #conv layer, activation then pooling\n        x = self.pool(F.relu(self.conv2(x)))\n        \n        x = x.view(-1, 16*5*5) # flatten\n        x = F.relu(self.fc1(x)) #layer1\n        x = F.relu(self.fc2(x)) #layer2\n        x = self.fc3(x) #layer3\n        \n        return x\n        "},"cell_type":"code","id":"a4e18df2-6906-4f37-a006-6220c5047f70","execution_count":76,"outputs":[]},{"source":"# create model \nmodel = ConvNet()","metadata":{"executionTime":51,"lastSuccessfullyExecutedCode":"# create model \nmodel = ConvNet()"},"cell_type":"code","id":"4ddfd769-cbb1-4d79-8b68-d29a1049f58a","execution_count":77,"outputs":[]},{"source":"# loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"executionTime":30,"lastSuccessfullyExecutedCode":"# loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"},"cell_type":"code","id":"3a36cc25-da2c-4eda-bc5e-bbe70a6edccf","execution_count":79,"outputs":[]},{"source":"# training loop\nn_total_steps = len(train_loader)\n\nfor epoch in range(num_epochs):\n    for i , (images, labels) in enumerate(train_loader):\n        \n        # forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 2000 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], step [{i+1}/{n_total_steps}], loss:{loss.item():.4f}')\n            \nprint('Finished Training')","metadata":{"executionTime":420593,"lastSuccessfullyExecutedCode":"# training loop\nn_total_steps = len(train_loader)\n\nfor epoch in range(num_epochs):\n    for i , (images, labels) in enumerate(train_loader):\n        \n        # forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 2000 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], step [{i+1}/{n_total_steps}], loss:{loss.item():.4f}')\n            \nprint('Finished Training')"},"cell_type":"code","id":"0b46ffcd-4db9-4230-8d26-bd619413a5bc","execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch [1/4], step [2000/12500], loss:2.3176\nEpoch [1/4], step [4000/12500], loss:2.2849\nEpoch [1/4], step [6000/12500], loss:2.2624\nEpoch [1/4], step [8000/12500], loss:2.1707\nEpoch [1/4], step [10000/12500], loss:2.0597\nEpoch [1/4], step [12000/12500], loss:1.9619\nEpoch [2/4], step [2000/12500], loss:1.6506\nEpoch [2/4], step [4000/12500], loss:2.1115\nEpoch [2/4], step [6000/12500], loss:1.5690\nEpoch [2/4], step [8000/12500], loss:1.8880\nEpoch [2/4], step [10000/12500], loss:1.7172\nEpoch [2/4], step [12000/12500], loss:1.5701\nEpoch [3/4], step [2000/12500], loss:1.1859\nEpoch [3/4], step [4000/12500], loss:1.7065\nEpoch [3/4], step [6000/12500], loss:1.5994\nEpoch [3/4], step [8000/12500], loss:1.9572\nEpoch [3/4], step [10000/12500], loss:0.6572\nEpoch [3/4], step [12000/12500], loss:1.9874\nEpoch [4/4], step [2000/12500], loss:0.7986\nEpoch [4/4], step [4000/12500], loss:0.7303\nEpoch [4/4], step [6000/12500], loss:1.5563\nEpoch [4/4], step [8000/12500], loss:1.2510\nEpoch [4/4], step [10000/12500], loss:3.4327\nEpoch [4/4], step [12000/12500], loss:1.1091\nFinished Training\n"}]},{"source":"# testing and evaluation\nwith torch.no_grad():\n    n_correct =0\n    n_samples =0\n    n_class_correct =[0 for i in range(10)]\n    n_class_samples =[0 for i in range(10)]\n    \n    for images, labels in test_loader:\n        # forward -- softmax prediction \n        outputs = model(images)\n        \n        # actual prediction\n        # value, index\n        _, predictions = torch.max(outputs, 1) # multiclass pred\n        n_samples += labels.shape[0]\n        n_correct += (predictions==labels).sum().item()\n        \n    acc = 100.0 * n_correct / n_samples\n    print(acc)","metadata":{"executionTime":12522,"lastSuccessfullyExecutedCode":"# testing and evaluation\nwith torch.no_grad():\n    n_correct =0\n    n_samples =0\n    n_class_correct =[0 for i in range(10)]\n    n_class_samples =[0 for i in range(10)]\n    \n    for images, labels in test_loader:\n        # forward -- softmax prediction \n        outputs = model(images)\n        \n        # actual prediction\n        # value, index\n        _, predictions = torch.max(outputs, 1) # multiclass pred\n        n_samples += labels.shape[0]\n        n_correct += (predictions==labels).sum().item()\n        \n    acc = 100.0 * n_correct / n_samples\n    print(acc)"},"cell_type":"code","id":"c9d11f5e-c7ad-452f-9177-715883529796","execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":"47.68\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}