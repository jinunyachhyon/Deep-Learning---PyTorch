{"cells":[{"source":"# Torch Basic","metadata":{},"cell_type":"markdown","id":"d43ee553-1a2b-4a7c-905f-e0e7d986bdf6"},{"source":"import torch ","metadata":{"id":"bA5ajAmk7XH6","executionTime":751,"lastSuccessfullyExecutedCode":"import torch "},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":1,"outputs":[]},{"source":"x = torch.empty(1) #<--1D tensor with 1 value\nx","metadata":{"executionTime":84,"lastSuccessfullyExecutedCode":"x = torch.empty(1) #<--1D tensor with 1 value\nx"},"cell_type":"code","id":"879dc8ae-2fc1-43c3-af07-ae3430461d00","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"tensor([16.9906])"},"metadata":{}}]},{"source":"x = torch.empty(3) #1D tensor with 3 value\nx","metadata":{"executionTime":48,"lastSuccessfullyExecutedCode":"x = torch.empty(3) #1D tensor with 3 value\nx"},"cell_type":"code","id":"970c5cf4-cc63-4651-8259-ea8113af904b","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"tensor([1.6990e+01, 4.5621e-41, 3.8723e-35])"},"metadata":{}}]},{"source":"x = torch.empty(2,3) #2D\nx","metadata":{"executionTime":68,"lastSuccessfullyExecutedCode":"x = torch.empty(2,3) #2D\nx"},"cell_type":"code","id":"876c52cd-c7a5-4a70-b854-4ca7b142b55c","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"tensor([[1.4605e-37, 0.0000e+00, 1.4604e-37],\n        [0.0000e+00, 8.9683e-44, 0.0000e+00]])"},"metadata":{}}]},{"source":"# random\nx = torch.rand(4,7)\nx","metadata":{"executionTime":61,"lastSuccessfullyExecutedCode":"# random\nx = torch.rand(4,7)\nx"},"cell_type":"code","id":"a6cc4b94-3031-485c-9666-14de4232d438","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"tensor([[0.2146, 0.2470, 0.1729, 0.7160, 0.5653, 0.6465, 0.6296],\n        [0.4632, 0.6054, 0.9097, 0.8697, 0.7862, 0.1525, 0.0710],\n        [0.0489, 0.8209, 0.0131, 0.6116, 0.3806, 0.7269, 0.3091],\n        [0.5920, 0.7559, 0.0421, 0.8701, 0.2036, 0.6484, 0.9474]])"},"metadata":{}}]},{"source":"#dtype \nx.dtype","metadata":{"executionTime":67,"lastSuccessfullyExecutedCode":"#dtype \nx.dtype"},"cell_type":"code","id":"5197dba0-0db3-45f7-8596-5df52e6a159c","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"torch.float32"},"metadata":{}}]},{"source":"#size\nx.size()","metadata":{"executionTime":60,"lastSuccessfullyExecutedCode":"#size\nx.size()"},"cell_type":"code","id":"6a145db4-b75e-46f7-83d1-c57734b91edb","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"torch.Size([4, 7])"},"metadata":{}}]},{"source":"# creating tensor with list\ntorch.tensor([2.5,0.1])","metadata":{"executionTime":48,"lastSuccessfullyExecutedCode":"# creating tensor with list\ntorch.tensor([2.5,0.1])"},"cell_type":"code","id":"167e613f-9b88-47bc-8997-39d9cefd6b7c","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"tensor([2.5000, 0.1000])"},"metadata":{}}]},{"source":"# Basic operations\nx =torch.rand(2,2)\ny =torch.rand(2,2)\nx,y","metadata":{"executionTime":61,"lastSuccessfullyExecutedCode":"# Basic operations\nx =torch.rand(2,2)\ny =torch.rand(2,2)\nx,y"},"cell_type":"code","id":"fcb15067-97d0-4f45-baf7-cff15865b194","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(tensor([[0.9300, 0.0638],\n         [0.3405, 0.2974]]),\n tensor([[0.9454, 0.1601],\n         [0.6468, 0.2262]]))"},"metadata":{}}]},{"source":"#addition\nx+y # performs element wise addition ","metadata":{"executionTime":26,"lastSuccessfullyExecutedCode":"#addition\nx+y # performs element wise addition "},"cell_type":"code","id":"00fc17d2-86c1-466a-b411-b021bf665f31","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"tensor([[1.8754, 0.2239],\n        [0.9874, 0.5236]])"},"metadata":{}}]},{"source":"# or perform addition with method\ntorch.add(x,y)","metadata":{"executionTime":48,"lastSuccessfullyExecutedCode":"# or perform addition with method\ntorch.add(x,y)"},"cell_type":"code","id":"3dd59637-0534-4578-9785-d9edd2a95156","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"tensor([[1.8754, 0.2239],\n        [0.9874, 0.5236]])"},"metadata":{}}]},{"source":"# inplace addition\ny.add_(x) #<--in pytorch all functions with trailing _(underscore) will do an in-place operation \n","metadata":{"executionTime":60,"lastSuccessfullyExecutedCode":"# inplace addition\ny.add_(x) #<--in pytorch all functions with trailing _(underscore) will do an in-place operation \n"},"cell_type":"code","id":"a4360d95-f0d4-4a2d-926b-1169657a6c8a","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"tensor([[1.8754, 0.2239],\n        [0.9874, 0.5236]])"},"metadata":{}}]},{"source":"# indexing and slicing \nx=torch.rand(5,3)\nprint(x)\nprint(x[:,0])","metadata":{"executionTime":54,"lastSuccessfullyExecutedCode":"# indexing and slicing \nx=torch.rand(5,3)\nprint(x)\nprint(x[:,0])"},"cell_type":"code","id":"171d14ef-e5f4-4264-8fd7-b5b1de9188c0","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[0.9340, 0.8160, 0.1577],\n        [0.8966, 0.7506, 0.4116],\n        [0.1751, 0.0521, 0.6028],\n        [0.8982, 0.4156, 0.3321],\n        [0.2017, 0.3600, 0.1515]])\ntensor([0.9340, 0.8966, 0.1751, 0.8982, 0.2017])\n"}]},{"source":"# .item() method to get the actual value\nprint(x[1,1])\nprint(x[1,1].item())","metadata":{"executionTime":49,"lastSuccessfullyExecutedCode":"# .item() method to get the actual value\nprint(x[1,1])\nprint(x[1,1].item())"},"cell_type":"code","id":"4cb7cc92-23b2-4f0b-9771-86ad2eec9761","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor(0.7506)\n0.7506189346313477\n"}]},{"source":"#reshaping tensor\nx = torch.rand(4,4) #<-- 2D\nprint(x)\ny = x.view(16) #<--1D\nprint(y)","metadata":{"executionTime":50,"lastSuccessfullyExecutedCode":"#reshaping tensor\nx = torch.rand(4,4) #<-- 2D\nprint(x)\ny = x.view(16) #<--1D\nprint(y)"},"cell_type":"code","id":"d43ccd42-cf80-47e4-96c5-e9e3f9c84b53","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[0.2248, 0.9213, 0.8510, 0.7273],\n        [0.6445, 0.9999, 0.7064, 0.2928],\n        [0.6696, 0.5272, 0.5626, 0.4283],\n        [0.5000, 0.5222, 0.1006, 0.3698]])\ntensor([0.2248, 0.9213, 0.8510, 0.7273, 0.6445, 0.9999, 0.7064, 0.2928, 0.6696,\n        0.5272, 0.5626, 0.4283, 0.5000, 0.5222, 0.1006, 0.3698])\n"}]},{"source":"# numpy to tensor and viceversa\nimport numpy as np\n\na=torch.ones(5)\nprint(a)\n\nb=a.numpy()\nprint(type(b))","metadata":{"executionTime":57,"lastSuccessfullyExecutedCode":"# numpy to tensor and viceversa\nimport numpy as np\n\na=torch.ones(5)\nprint(a)\n\nb=a.numpy()\nprint(type(b))"},"cell_type":"code","id":"d1b077bc-56bd-4451-a97e-e276404beff1","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([1., 1., 1., 1., 1.])\n<class 'numpy.ndarray'>\n"}]},{"source":"a.add_(1)","metadata":{"executionTime":54,"lastSuccessfullyExecutedCode":"a.add_(1)"},"cell_type":"code","id":"b8c0b11e-ee96-4de5-9aa5-38340fc3e7c9","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"tensor([2., 2., 2., 2., 2.])"},"metadata":{}}]},{"source":"b","metadata":{"executionTime":133,"lastSuccessfullyExecutedCode":"b"},"cell_type":"code","id":"bc78d6f1-84a0-49bb-bfa1-d4190b0e917c","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"array([2., 2., 2., 2., 2.], dtype=float32)"},"metadata":{}}]},{"source":"changing 'a' changes 'b' because they both point to same memory location","metadata":{},"cell_type":"markdown","id":"d3ee56c9-cd81-49f3-8a14-d3fdf59c3caa"},{"source":"#viceversa\n\na=np.ones(5)\nprint(a)\n\nb=torch.from_numpy(a)\nprint(b)","metadata":{"executionTime":97,"lastSuccessfullyExecutedCode":"#viceversa\n\na=np.ones(5)\nprint(a)\n\nb=torch.from_numpy(a)\nprint(b)"},"cell_type":"code","id":"c97fb0ea-358e-4101-8ae8-43b1af89dd96","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":"[1. 1. 1. 1. 1.]\ntensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"}]},{"source":"# Gradient calculation with Autograd","metadata":{},"cell_type":"markdown","id":"98b16804-608f-4972-ad04-c10c3d697e66"},{"source":"x = torch.randn(3, requires_grad=True) #<-- to calculate gradient of some function with resp. to x\nx #<-- whenever we do operations with this tensor, it creates a computational graph ","metadata":{"executionTime":56,"lastSuccessfullyExecutedCode":"x = torch.randn(3, requires_grad=True) #<-- to calculate gradient of some function with resp. to x\nx #<-- whenever we do operations with this tensor, it creates a computational graph "},"cell_type":"code","id":"13efcacd-4e1e-4eba-b0f8-1bdc06df91a9","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"tensor([-1.0857,  0.4361, -0.8030], requires_grad=True)"},"metadata":{}}]},{"source":"y = x+2 #<-- creates computational graph\ny","metadata":{"executionTime":56,"lastSuccessfullyExecutedCode":"y = x+2 #<-- creates computational graph\ny"},"cell_type":"code","id":"9355657e-8709-42f5-974e-d71ee8843724","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"tensor([0.9143, 2.4361, 1.1970], grad_fn=<AddBackward0>)"},"metadata":{}}]},{"source":"pytorch automatically create and store a function for us which is used in back propagation and to get the gradients","metadata":{},"cell_type":"markdown","id":"b7fe415c-dda5-4907-941e-af212697963e"},{"source":"![image](image.png)\n","metadata":{},"cell_type":"markdown","id":"a504f796-880c-4c2a-b9fd-972ac452dd36"},{"source":"z = y*y*2\nz","metadata":{"executionTime":69,"lastSuccessfullyExecutedCode":"z = y*y*2\nz"},"cell_type":"code","id":"5f828911-0251-4528-bf57-92fb1002d857","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"tensor([ 1.6718, 11.8695,  2.8657], grad_fn=<MulBackward0>)"},"metadata":{}}]},{"source":"z = z.mean()","metadata":{"executionTime":80,"lastSuccessfullyExecutedCode":"z = z.mean()"},"cell_type":"code","id":"1d1e79b5-926f-47f2-9dc0-5586c321148b","execution_count":33,"outputs":[]},{"source":"z","metadata":{"executionTime":82,"lastSuccessfullyExecutedCode":"z"},"cell_type":"code","id":"b3d35919-9e49-4825-8c05-5ac34ab0e06c","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"tensor(5.4690, grad_fn=<MeanBackward0>)"},"metadata":{}}]},{"source":"# calculate the gardients \nz.backward() #dz/dx\nx.grad #<stores gradient","metadata":{"executionTime":90,"lastSuccessfullyExecutedCode":"# calculate the gardients \nz.backward() #dz/dx\nx.grad #<stores gradient"},"cell_type":"code","id":"ff78b208-842d-4016-b303-d39bb17f078c","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"tensor([1.2190, 3.2482, 1.5960])"},"metadata":{}}]},{"source":"z = y*y*2\nz","metadata":{"executionTime":55,"lastSuccessfullyExecutedCode":"z = y*y*2\nz"},"cell_type":"code","id":"2d6c0dd7-fdce-4349-b2fb-4398b85365ab","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"tensor([ 1.6718, 11.8695,  2.8657], grad_fn=<MulBackward0>)"},"metadata":{}}]},{"source":"#if calculated gradient, throes error because grad can be implicitly created only for scalr output\n\n# so create vector of sam size\nv = torch.tensor([0.1,1.0,0.001],dtype=torch.float32)\nz.backward(v)","metadata":{"executionTime":80,"lastSuccessfullyExecutedCode":"#if calculated gradient, throes error because grad can be implicitly created only for scalr output\n\n# so create vector of sam size\nv = torch.tensor([0.1,1.0,0.001],dtype=torch.float32)\nz.backward(v)"},"cell_type":"code","id":"2c8a0b90-ca73-4d7b-92f1-4006adb817d4","execution_count":38,"outputs":[]},{"source":"x.grad","metadata":{"executionTime":71,"lastSuccessfullyExecutedCode":"x.grad"},"cell_type":"code","id":"5a0112e2-80e5-430c-9ba5-69a4a97d8776","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"tensor([ 1.5848, 12.9927,  1.6008])"},"metadata":{}}]},{"source":"## avoid torch to keep history\nx.requires_grad_(False) #<--method 1\nx","metadata":{"executionTime":58,"lastSuccessfullyExecutedCode":"## avoid torch to keep history\nx.requires_grad_(False) #<--method 1\nx"},"cell_type":"code","id":"4e310b02-ba97-4769-8737-b2f604b8f261","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"tensor([-1.0857,  0.4361, -0.8030])"},"metadata":{}}]},{"source":"y = x.detach() #method 2\ny","metadata":{"executionTime":46,"lastSuccessfullyExecutedCode":"y = x.detach() #method 2\ny"},"cell_type":"code","id":"465bc7a9-a91a-4882-90e6-1adb44529aff","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"tensor([-1.0857,  0.4361, -0.8030])"},"metadata":{}}]},{"source":"with torch.no_grad(): #method 3\n    y = x+2\n    print(y)","metadata":{"executionTime":97,"lastSuccessfullyExecutedCode":"with torch.no_grad(): #method 3\n    y = x+2\n    print(y)"},"cell_type":"code","id":"c5ea40d2-3a5d-4d36-ad29-9a9062df4ec8","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([0.9143, 2.4361, 1.1970])\n"}]},{"source":"## training example\n\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(2):\n    model_output = (weights*3).sum()\n    print(model_output) #<-- scalar output for grad \n    \n    model_output.backward() #<-- second backward call with again accumulate the values and write them into grad attribute\n    \n    print(weights.grad)","metadata":{"executionTime":70,"lastSuccessfullyExecutedCode":"## training example\n\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(2):\n    model_output = (weights*3).sum()\n    print(model_output) #<-- scalar output for grad \n    \n    model_output.backward() #<-- second backward call with again accumulate the values and write them into grad attribute\n    \n    print(weights.grad)"},"cell_type":"code","id":"d55ad712-e847-4b0e-94ab-375cb239e0f4","execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor(12., grad_fn=<SumBackward0>)\ntensor([3., 3., 3., 3.])\ntensor(12., grad_fn=<SumBackward0>)\ntensor([6., 6., 6., 6.])\n"}]},{"source":"the gradients are clearly incorrect","metadata":{},"cell_type":"markdown","id":"41149b8d-e5e5-4aaf-882a-248b11bcc81c"},{"source":"# before optimization clearing gradients\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(2):\n    model_output = (weights*3).sum()\n    print(model_output) #<-- scalar output for grad \n    \n    model_output.backward() #<-- second backward call with again accumulate the values and write them into grad attribute\n    \n    print(weights.grad)\n    \n    weights.grad.zero_()","metadata":{"executionTime":58,"lastSuccessfullyExecutedCode":"# before optimization clearing gradients\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(2):\n    model_output = (weights*3).sum()\n    print(model_output) #<-- scalar output for grad \n    \n    model_output.backward() #<-- second backward call with again accumulate the values and write them into grad attribute\n    \n    print(weights.grad)\n    \n    weights.grad.zero_()"},"cell_type":"code","id":"936d1eb5-04df-4d09-ac9a-fac195790570","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor(12., grad_fn=<SumBackward0>)\ntensor([3., 3., 3., 3.])\ntensor(12., grad_fn=<SumBackward0>)\ntensor([3., 3., 3., 3.])\n"}]},{"source":"# Backpropagartion with pytorch\n![image-2](image-2.png)\n","metadata":{},"cell_type":"markdown","id":"f74a7447-a0e4-4e90-a6af-92beb6021cbc"},{"source":"import torch\n\nx = torch.tensor(1.0)\ny = torch.tensor(2.0)\nw = torch.tensor(1.0, requires_grad=True)\n\n#forward pass and compute the loss\ny_hat = w*x\nloss = (y_hat - y)**2\n\nprint(loss)","metadata":{"executionTime":852,"lastSuccessfullyExecutedCode":"import torch\n\nx = torch.tensor(1.0)\ny = torch.tensor(2.0)\nw = torch.tensor(1.0, requires_grad=True)\n\n#forward pass and compute the loss\ny_hat = w*x\nloss = (y_hat - y)**2\n\nprint(loss)"},"cell_type":"code","id":"249fd702-3d3d-4ecd-8714-0e3c74ef6a84","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor(1., grad_fn=<PowBackward0>)\n"}]},{"source":"# backward pass\nloss.backward()\nw.grad","metadata":{"executionTime":83,"lastSuccessfullyExecutedCode":"# backward pass\nloss.backward()\nw.grad"},"cell_type":"code","id":"7a23dfe0-145c-4e5b-9b20-70ad3f3c966e","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"tensor(-2.)"},"metadata":{}}]},{"source":"# Gradient descent and Backpropagation\n\n- Prediction: PyTorch Model\n- Gradients Computation: Autograd\n- Loss Computation: PyTorch Loss\n- Parameter updates: PyTorch Optimizer","metadata":{},"cell_type":"markdown","id":"c60070d0-3aeb-467f-aaed-c5d7d78c40f5"},{"source":"A cost function estimates the error of a model.\nGradient descent is a technique that uses derivative of the cost function\nto change the parameter values (weights / co-efficients) to minimize the \ncost or error.\nChange the current weights by delta and take a step. The direction and size \nof step is calculated by the gradient (slope) of cost function at the \ncurrent position by some specified learning rate. The gradient vector \ncontains the slopes for all weight vectors / co-efficients. This gradient \nvector is used to update all existing weights.\nIf slope is negative, we are stepping downhill and will reach a \nminimum position when the slope becomes 0 / algorithm converges.\nIf slope is positive, we are stepping uphill and will reach a \nmaximum position when the slope becomes 0 / algorithm converges.\n\nProcess of applying gradient descent:\n1. Initialize parameters randomly\n2. Calculate cost for training set with a cost function\n3. Calculate gradient of the cost function (partial derivative for all dataset)\n4. Update weights with new values\n5. Repeat from step 2 until cost is small enough","metadata":{},"cell_type":"markdown","id":"07b7c907-8071-4aa9-a296-3afccdc488ba"},{"source":"import torch","metadata":{"executionTime":742,"lastSuccessfullyExecutedCode":"import torch"},"cell_type":"code","id":"27b644c0-bede-43e6-a3d5-1a75adcdaa5c","execution_count":1,"outputs":[]},{"source":"import numpy as np\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\nx=np.array([1,2,3,4],dtype=np.float32)\n\n# actual value \ny=np.array([2,4,6,8],dtype=np.float32) \n\n#intialize weight (randomly)\nw=0.0","metadata":{"executionTime":36,"lastSuccessfullyExecutedCode":"import numpy as np\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\nx=np.array([1,2,3,4],dtype=np.float32)\n\n# actual value \ny=np.array([2,4,6,8],dtype=np.float32) \n\n#intialize weight (randomly)\nw=0.0"},"cell_type":"code","id":"0cc0f6c6-6d12-4b31-a52b-80f0f0a57705","execution_count":12,"outputs":[]},{"source":"# model prediction (returns y_predicted)\ndef forward(x):\n    return w*x\n\n# loss = MSE\ndef loss(y,y_predicted):\n    return ((y_predicted - y)**2).mean()\n\n# gradient\n# MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N * 2x(wx - y)\ndef gradient(x,y,y_predicted):\n    return np.dot(2*x, y_predicted - y).mean()","metadata":{"executionTime":79,"lastSuccessfullyExecutedCode":"# model prediction (returns y_predicted)\ndef forward(x):\n    return w*x\n\n# loss = MSE\ndef loss(y,y_predicted):\n    return ((y_predicted - y)**2).mean()\n\n# gradient\n# MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N * 2x(wx - y)\ndef gradient(x,y,y_predicted):\n    return np.dot(2*x, y_predicted - y).mean()"},"cell_type":"code","id":"8761c73d-83dd-4268-b4b8-f7c097a12918","execution_count":13,"outputs":[]},{"source":"print(f'Prediction before training: f(5) = {forward(5):.3f}')","metadata":{"executionTime":39,"lastSuccessfullyExecutedCode":"print(f'Prediction before training: f(5) = {forward(5):.3f}')"},"cell_type":"code","id":"db95593c-842e-4f8d-8799-84bfb7d11cf9","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"Prediction before training: f(5) = 0.000\n"}]},{"source":"#Training \nlearning_rate = 0.01\nn_iters =10\n\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = forward(x)\n    \n    #loss\n    l = loss(y,y_pred)\n    \n    #gradients\n    dw = gradient(x,y,y_pred)\n    \n    #update weights\n    w -= learning_rate *dw\n    \n    if epoch %1 ==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss={l:.8f}')\n        \nprint(f'Prediction after training: f(5) = {forward(5):.3f}')","metadata":{"executionTime":90,"lastSuccessfullyExecutedCode":"#Training \nlearning_rate = 0.01\nn_iters =10\n\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = forward(x)\n    \n    #loss\n    l = loss(y,y_pred)\n    \n    #gradients\n    dw = gradient(x,y,y_pred)\n    \n    #update weights\n    w -= learning_rate *dw\n    \n    if epoch %1 ==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss={l:.8f}')\n        \nprint(f'Prediction after training: f(5) = {forward(5):.3f}')"},"cell_type":"code","id":"b42154e9-990b-4f31-bcb7-eee334dc2b8a","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"epoch 1: w = 1.200, loss=30.00000000\nepoch 2: w = 1.680, loss=4.79999924\nepoch 3: w = 1.872, loss=0.76800019\nepoch 4: w = 1.949, loss=0.12288000\nepoch 5: w = 1.980, loss=0.01966083\nepoch 6: w = 1.992, loss=0.00314574\nepoch 7: w = 1.997, loss=0.00050331\nepoch 8: w = 1.999, loss=0.00008053\nepoch 9: w = 1.999, loss=0.00001288\nepoch 10: w = 2.000, loss=0.00000206\nPrediction after training: f(5) = 9.999\n"}]},{"source":"## Implementing in torch","metadata":{},"cell_type":"markdown","id":"6d792766-a26d-4db3-b084-f21af55c4dd5"},{"source":"import torch\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\nx=torch.tensor([1,2,3,4],dtype=torch.float32)\n\n# actual value \ny=torch.tensor([2,4,6,8],dtype=torch.float32) \n\n#intialize weight (randomly)\nw=torch.tensor(0.0,dtype=torch.float32, requires_grad =True)","metadata":{"executionTime":27,"lastSuccessfullyExecutedCode":"import torch\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\nx=torch.tensor([1,2,3,4],dtype=torch.float32)\n\n# actual value \ny=torch.tensor([2,4,6,8],dtype=torch.float32) \n\n#intialize weight (randomly)\nw=torch.tensor(0.0,dtype=torch.float32, requires_grad =True)"},"cell_type":"code","id":"2ebd174d-ae91-43a5-b555-9a1ca6d25211","execution_count":21,"outputs":[]},{"source":"# model prediction (returns y_predicted)\ndef forward(x):\n    return w*x\n\n# loss = MSE\ndef loss(y,y_predicted):\n    return ((y_predicted - y)**2).mean()\n","metadata":{"executionTime":40,"lastSuccessfullyExecutedCode":"# model prediction (returns y_predicted)\ndef forward(x):\n    return w*x\n\n# loss = MSE\ndef loss(y,y_predicted):\n    return ((y_predicted - y)**2).mean()\n"},"cell_type":"code","id":"7a521c63-a023-4aeb-9ae1-dc5aba630c48","execution_count":22,"outputs":[]},{"source":"#Training \nlearning_rate = 0.01\nn_iters =10\n\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = forward(x)\n    \n    #loss\n    l = loss(y,y_pred)\n    \n    #gradients = backward pass\n    l.backward() #dl/dw\n    \n    #update weights\n    with torch.no_grad():\n        w -= learning_rate *w.grad\n        \n    #zero gradients\n    w.grad.zero_()\n    \n    if epoch %1 ==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss={l:.8f}')\n        \nprint(f'Prediction after training: f(5) = {forward(5):.3f}')","metadata":{"executionTime":49,"lastSuccessfullyExecutedCode":"#Training \nlearning_rate = 0.01\nn_iters =10\n\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = forward(x)\n    \n    #loss\n    l = loss(y,y_pred)\n    \n    #gradients = backward pass\n    l.backward() #dl/dw\n    \n    #update weights\n    with torch.no_grad():\n        w -= learning_rate *w.grad\n        \n    #zero gradients\n    w.grad.zero_()\n    \n    if epoch %1 ==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss={l:.8f}')\n        \nprint(f'Prediction after training: f(5) = {forward(5):.3f}')"},"cell_type":"code","id":"57065de9-87b4-4e1d-be39-0c5cd3faa5fc","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"epoch 1: w = 0.300, loss=30.00000000\nepoch 2: w = 0.555, loss=21.67499924\nepoch 3: w = 0.772, loss=15.66018772\nepoch 4: w = 0.956, loss=11.31448650\nepoch 5: w = 1.113, loss=8.17471695\nepoch 6: w = 1.246, loss=5.90623236\nepoch 7: w = 1.359, loss=4.26725292\nepoch 8: w = 1.455, loss=3.08308983\nepoch 9: w = 1.537, loss=2.22753215\nepoch 10: w = 1.606, loss=1.60939169\nPrediction after training: f(5) = 8.031\n"}]},{"source":"# Complete PyTorch pipeline","metadata":{},"cell_type":"markdown","id":"6d972ecb-d72c-4128-bafd-4d754ce73620"},{"source":"# 1) Design model (input, output size, forward pass)\n# 2) Construct loss and optimizer\n# 3) Training loop\n#   - forward pass: compute prediction\n#   - backward pass: gradients\n#   - update weights\n\nimport torch","metadata":{"executionTime":411,"lastSuccessfullyExecutedCode":"# 1) Design model (input, output size, forward pass)\n# 2) Construct loss and optimizer\n# 3) Training loop\n#   - forward pass: compute prediction\n#   - backward pass: gradients\n#   - update weights\n\nimport torch"},"cell_type":"code","id":"54b599d7-c158-4d44-98e3-f94b78167c38","execution_count":24,"outputs":[]},{"source":"# nn module\nimport torch.nn as nn\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\nx=torch.tensor([1,2,3,4],dtype=torch.float32)\n\n# actual value \ny=torch.tensor([2,4,6,8],dtype=torch.float32) \n\n#intialize weight (randomly)\nw=torch.tensor(0.0,dtype=torch.float32, requires_grad =True)\n\n# model prediction (returns y_predicted)\ndef forward(x):\n    return w*x\n\n#Training \nlearning_rate = 0.01\nn_iters =100\n\n# loss and optimizer\nloss = nn.MSELoss() #<-- callable fucntion\noptimizer = torch.optim.SGD([w], lr=learning_rate) #<-- automatically update weights\n\n## training loop\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = forward(x)\n    \n    #loss\n    l = loss(y,y_pred) #<--pass to nn callable function to calc. loss\n    \n    #gradients = backward pass\n    l.backward() #dl/dw\n    \n    #update weights\n    # (w -= learning_rate *w.grad) using optimizer\n    optimizer.step() \n        \n    #zero gradients\n    # w.grad.zero_() using optimizer\n    optimizer.zero_grad()\n    \n    if epoch %10 ==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss={l:.8f}')\n        \nprint(f'Prediction after training: f(5) = {forward(5):.3f}')","metadata":{"executionTime":46,"lastSuccessfullyExecutedCode":"# nn module\nimport torch.nn as nn\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\nx=torch.tensor([1,2,3,4],dtype=torch.float32)\n\n# actual value \ny=torch.tensor([2,4,6,8],dtype=torch.float32) \n\n#intialize weight (randomly)\nw=torch.tensor(0.0,dtype=torch.float32, requires_grad =True)\n\n# model prediction (returns y_predicted)\ndef forward(x):\n    return w*x\n\n#Training \nlearning_rate = 0.01\nn_iters =100\n\n# loss and optimizer\nloss = nn.MSELoss() #<-- callable fucntion\noptimizer = torch.optim.SGD([w], lr=learning_rate) #<-- automatically update weights\n\n## training loop\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = forward(x)\n    \n    #loss\n    l = loss(y,y_pred) #<--pass to nn callable function to calc. loss\n    \n    #gradients = backward pass\n    l.backward() #dl/dw\n    \n    #update weights\n    # (w -= learning_rate *w.grad) using optimizer\n    optimizer.step() \n        \n    #zero gradients\n    # w.grad.zero_() using optimizer\n    optimizer.zero_grad()\n    \n    if epoch %10 ==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss={l:.8f}')\n        \nprint(f'Prediction after training: f(5) = {forward(5):.3f}')"},"cell_type":"code","id":"ed177704-e14d-4ec8-8d10-4ce18be70329","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"epoch 1: w = 0.300, loss=30.00000000\nepoch 11: w = 1.665, loss=1.16278565\nepoch 21: w = 1.934, loss=0.04506890\nepoch 31: w = 1.987, loss=0.00174685\nepoch 41: w = 1.997, loss=0.00006770\nepoch 51: w = 1.999, loss=0.00000262\nepoch 61: w = 2.000, loss=0.00000010\nepoch 71: w = 2.000, loss=0.00000000\nepoch 81: w = 2.000, loss=0.00000000\nepoch 91: w = 2.000, loss=0.00000000\nPrediction after training: f(5) = 10.000\n"}]},{"source":"## replace manually implemented forward method with pytorch model\n\nimport torch\n# nn module\nimport torch.nn as nn\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\n\n# x,y should be different shape -- 2D \nx=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32) # 4 samples, 1 feature\n# actual value \ny=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32) \n\nn_samples, n_features = x.shape\nprint(n_samples, n_features)\n\n# pytorch model\ninput_size = n_features\noutput_size = n_features\nmodel = nn.Linear(input_size,output_size) #model creation\n\n#Training \nlearning_rate = 0.01\nn_iters =100\n\n# loss and optimizer\nloss = nn.MSELoss() #<-- callable fucntion\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #<-- automatically update weights\n\n## training loop\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = model(x)\n    \n    #loss\n    l = loss(y,y_pred) #<--pass to nn callable function to calc. loss\n    \n    #gradients = backward pass\n    l.backward() #dl/dw\n    \n    #update weights\n    # (w -= learning_rate *w.grad) using optimizer\n    optimizer.step() \n        \n    #zero gradients\n    # w.grad.zero_() using optimizer\n    optimizer.zero_grad()\n    \n    if epoch %10 ==0:\n        [w,b] = model.parameters()\n        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss={l:.8f}')\n   \nX_test = torch.tensor([5],dtype=torch.float32)\nprint(f'Prediction after training: f(5) = {model(X_test).item():.3f}')","metadata":{"executionTime":258,"lastSuccessfullyExecutedCode":"## replace manually implemented forward method with pytorch model\n\nimport torch\n# nn module\nimport torch.nn as nn\n\n# f = w*x\n\n# suppose weight=2 --> f=2*x\n\n# x,y should be different shape -- 2D \nx=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32) # 4 samples, 1 feature\n# actual value \ny=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32) \n\nn_samples, n_features = x.shape\nprint(n_samples, n_features)\n\n# pytorch model\ninput_size = n_features\noutput_size = n_features\nmodel = nn.Linear(input_size,output_size) #model creation\n\n#Training \nlearning_rate = 0.01\nn_iters =100\n\n# loss and optimizer\nloss = nn.MSELoss() #<-- callable fucntion\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #<-- automatically update weights\n\n## training loop\nfor epoch in range(n_iters):\n    #prediction = forward pass\n    y_pred = model(x)\n    \n    #loss\n    l = loss(y,y_pred) #<--pass to nn callable function to calc. loss\n    \n    #gradients = backward pass\n    l.backward() #dl/dw\n    \n    #update weights\n    # (w -= learning_rate *w.grad) using optimizer\n    optimizer.step() \n        \n    #zero gradients\n    # w.grad.zero_() using optimizer\n    optimizer.zero_grad()\n    \n    if epoch %10 ==0:\n        [w,b] = model.parameters()\n        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss={l:.8f}')\n   \nX_test = torch.tensor([5],dtype=torch.float32)\nprint(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"},"cell_type":"code","id":"1bb60c6e-9617-4501-a70c-b5c622d2584b","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"4 1\nepoch 1: w = 0.924, loss=10.55867481\nepoch 11: w = 1.672, loss=0.32202727\nepoch 21: w = 1.797, loss=0.05433739\nepoch 31: w = 1.821, loss=0.04473386\nepoch 41: w = 1.829, loss=0.04196347\nepoch 51: w = 1.835, loss=0.03951670\nepoch 61: w = 1.840, loss=0.03721647\nepoch 71: w = 1.845, loss=0.03505031\nepoch 81: w = 1.849, loss=0.03301021\nepoch 91: w = 1.854, loss=0.03108880\nPrediction after training: f(5) = 9.707\n"}]},{"source":"[w,b] = model.parameters()\nw","metadata":{"executionTime":120,"lastSuccessfullyExecutedCode":"[w,b] = model.parameters()\nw"},"cell_type":"code","id":"5eb7e7f0-fb5f-448b-b27b-8117fa6a9d5b","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"Parameter containing:\ntensor([[1.8576]], requires_grad=True)"},"metadata":{}}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}