{"cells":[{"source":"# Activation Function\n![image-7](image-7.png)\n","metadata":{},"cell_type":"markdown","id":"a5c9b109-4ffb-41b8-ba18-ea4a6889f4be"},{"source":"### 1. Step Function\n![image-8](image-8.png)\n","metadata":{},"cell_type":"markdown","id":"350a3d08-c82c-4865-b30e-9b72b866ab1d"},{"source":"### 2. Sigmoid Function\n![image-9](image-9.png)\n\nTypically in the last layer of a binary classification problem","metadata":{},"cell_type":"markdown","id":"3f19cfd9-f161-42c5-b5a5-6dd0f262e800"},{"source":"### 3. TanH Function\n![image-10](image-10.png)\n\nHidden Layer","metadata":{},"cell_type":"markdown","id":"545fc0a3-3b94-4ab5-9298-91bf9ceea063"},{"source":"### 4. ReLU Function\n![image-11](image-11.png)\n","metadata":{},"cell_type":"markdown","id":"c238a505-873e-45b3-8eb2-63ecfccf5e0c"},{"source":"### 5. Leaky ReLU Function\n![image-12](image-12.png)\n\nSolves vanishing gradient problem","metadata":{},"cell_type":"markdown","id":"1f389176-1653-46ee-93de-f4d5a165939b"},{"source":"### 6. Softmax\n![image-13](image-13.png)\n\nLast layer of muticlass classification","metadata":{},"cell_type":"markdown","id":"28aafb57-b3dc-4a3f-9eff-66bd27e19b54"},{"source":"# option 1 (create nn module)\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(NeuralNet, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n        return out","metadata":{},"cell_type":"code","id":"92f3d6d6-5152-435f-80d3-e1e0facf5996","execution_count":null,"outputs":[]},{"source":"# option 2 (use activation functions directly in forward pass)\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(NeuralNet, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, 1)\n        \n    def forward(self,x):\n        out = torch.relu(self.linear1(x))\n        out = torch.sigmoid(self.linear2(out))\n        return out","metadata":{},"cell_type":"code","id":"8360b589-6b88-4846-af20-6f78db2d41a3","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}